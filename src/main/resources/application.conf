kafka {
  host = 127.0.0.1
  port = 9092

  bootstrap.servers = ${kafka.host}":"${kafka.port}
  zookeeper.connect = ${zookeeper.hosts}

  groupId = distinct-counter

  broker {
    listeners = "PLAINTEXT://"${kafka.host}":"${kafka.port}
    advertised.listeners = ${kafka.broker.listeners}
  }

  producer {
    bootstrap.servers = ${kafka.bootstrap.servers}
    key.serializer = org.apache.kafka.common.serialization.StringSerializer
    value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer
  }

  consumer {
    group.id = ${kafka.groupId}
    client.id = ${kafka.groupId}-client
    bootstrap.servers = ${kafka.bootstrap.servers}
    auto.offset.reset = earliest
    session.timeout.ms = 6000
    auto.commit.interval.ms = 1000
    heartbeat.interval.ms = 500
    enable.auto.commit = true
    max.poll.records = 50000
    key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
    value.deserializer = org.apache.kafka.common.serialization.ByteArrayDeserializer
  }
}

zookeeper {
  node = 127.0.0.1
  port = 2181
  hosts = ${zookeeper.node}":"${zookeeper.port}
  timeout {
    session: 1000
    connection: 1000
  }
  secure = false
}

source {
  path = data/stream.jsonl.gz
}

doorman {
  gid = doorman
  cid = ${doorman.gid}-%s
  ingress {
    uid = "[a-z0-9]{19}"
  }
  sticky = true
}

sink {
  gid = sink-${sink.name}-${sink.ext}
  cid = ${sink.gid}-${sink.uuid}
  name = card
  path = output/data/sink-${sink.name}.${sink.ext}
  ext = csv
  sticky = true
  uuid = override-me
  # set this property to indicate a collection of a metric.
  # A metric ( e.g frames_processed) helps to assess non-functional requirements
  metric = false
}

worker {
  gid = worker-${worker.property}-%s-${worker.window}
  cid = ${worker.gid}
  window = 1m
  method = PCSA
  sticky = true

  property = uid

  # set this property to indicate a collection of a metric.
  # A metric ( e.g frames_processed) helps to assess non-functional requirements
  metric = false
}

